{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rajpurkar # 데이터\n",
    "    \n",
    "https://medium.com/@eyfydsyd97/bert-for-question-answer-fine-tuning%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%98%EC%97%AC-by-pytorch-fbe15fdef330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Use SquAD 1.1 data\n",
    "#input은 [CLS]+[context]+[SEP]+[question]+[SEP]+[PAD]\n",
    "\n",
    "def text_processing(data, Max_len = 192):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attn_mask_ids = []\n",
    "    label_start = []\n",
    "    label_end = []\n",
    "    \n",
    "    for i in range(len(data)): \n",
    "        label_st = 0\n",
    "        label_ed = 0 \n",
    "        context = data['context'][i]\n",
    "        que = data['question'][i]\n",
    "        ans = data['answer'][i]\n",
    "        sent1 = tokenizer.encode(context, add_special_tokens=False, max_length=Max_len)\n",
    "\n",
    "    if len(sent1) < Max_len:\n",
    "        sent2 = tokenizer.encode(que, add_special_tokens=False)\n",
    "        sent3 = tokenizer.encode(ans, add_special_tokens=False)\n",
    "\n",
    "        len_ans = len(sent3)\n",
    "        for j in range(len(sent1)):\n",
    "            if sent1[j: j + len_ans] == sent3:\n",
    "                label_st = j + 1\n",
    "                label_ed = j + len_ans\n",
    "\n",
    "        length = len(sent1) + len(sent2) + 3\n",
    "        ids = [101] + sent1+ [102] + sent2 + [102] + [0]*(Max_len - length)\n",
    "        ids = ids[:Max_len]\n",
    "        type_ids = [0] * (len(sent1) + 2) + [1]*(Max_len - len(sent1) - 2) \n",
    "        type_ids = type_ids[:Max_len]\n",
    "        attn_ids = [1]*length + [0]*(Max_len - length)\n",
    "        attn_ids = attn_ids[:Max_len]\n",
    "\n",
    "        input_ids.append(torch.tensor(ids, dtype=torch.long))\n",
    "        token_type_ids.append(torch.tensor(type_ids, dtype=torch.long))\n",
    "        attn_mask_ids.append(torch.tensor(attn_ids, dtype=torch.long))\n",
    "        label_start.append(torch.tensor(label_st, dtype=torch.long))\n",
    "        label_end.append(torch.tensor(label_ed, dtype=torch.long))\n",
    "  \n",
    "    return input_ids, token_type_ids, attn_mask_ids, label_start, label_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_QA_dataset(Dataset):\n",
    "    def __init__(self, input_ids, token_type_ids, attn_mask_ids, label_start, label_end):\n",
    "        self.input = input_ids\n",
    "        self.token_type = token_type_ids\n",
    "        self.attn_mask = attn_mask_ids\n",
    "        self.label_start = label_start\n",
    "        self.label_end = label_end\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def __getitem(self, idx):\n",
    "        x = self.input[idx]\n",
    "        y = self.token_type[idx]\n",
    "        z = self.attn_mask[idx]\n",
    "        label_s = self.label_start[idx].view(-1)\n",
    "        label_e = self.label_end[idx].view(-1)\n",
    "        \n",
    "        return x, y, z, label_s, label_e\n",
    "    \n",
    "train_dataset = BERT_QA_dataset(input_ids, token_type_ids, attn_mask_idx, label_start, label_end)\n",
    "valid_dataset = BERT_QA_dataset(input_ids_v, token_type_ids_v, attn_mask_ids_v, label_start_v, label_end_v)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=Batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=Batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a5a892ef12da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBert_QA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BertModel' is not defined"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class Bert_QA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(778, 2)\n",
    "        \n",
    "    def froward(self, input_ids, token_type_ids, attn_mask_idx):\n",
    "        output, _ = self.bert(input_ids, attention_mask = attn_mask_ids, token_type_ids=token_type_ids)\n",
    "        x = self.dropout(output)\n",
    "        logits = self.linear(x)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        \n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(data_loader, model, optimizer, device, loss_fn):  \n",
    "    model.train()\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for bi, d in enumerate(tk0):\n",
    "        input_ids, token_type_ids, attn_mask_ids, label_start, label_end = d\n",
    "        input_ids = input_ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        attn_mask_ids = attn_mask_ids.to(device, dtype=torch.long)\n",
    "        label_start = label_start.to(device, dtype=torch.long)\n",
    "        label_end = label_end.to(device, dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "        start_logits, end_logits = model(input_ids, token_type_ids, attn_mask_ids)\n",
    "        loss = loss_fn(start_logits, end_logits, label_start,  label_end)\n",
    "        total_loss += loss.item()\n",
    "        if bi % 100 ==0:\n",
    "            print(f\"loss:{loss}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_train_loss = total_loss / len(data_loader) \n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))  \n",
    "    \n",
    "def eval_one_epoch(data_loader, model,  device, loss_fn):\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    fin_targets_st = []\n",
    "    fin_outputs_st = []\n",
    "    fin_targets_ed = []\n",
    "    fin_outputs_ed = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "    for bi, d in enumerate(tk0):\n",
    "        input_ids, token_type_ids, attn_mask_ids, label_start, label_end = d\n",
    "        input_ids = input_ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        attn_mask_ids = attn_mask_ids.to(device, dtype=torch.long)\n",
    "        label_start = label_start.to(device, dtype=torch.long)\n",
    "        label_end = label_end.to(device, dtype=torch.long)\n",
    "\n",
    "        logits_start, logits_end = model(input_ids, token_type_ids, attn_mask_ids)\n",
    "         loss = loss_fn(logits_start, logits_end, label_start,  label_end)\n",
    "\n",
    "        logits_start = logits_start.detach().cpu().numpy()\n",
    "        label_start = label_start.detach().cpu().numpy()\n",
    "        logits_end = logits_end.detach().cpu().numpy()\n",
    "        label_end = label_end.detach().cpu().numpy()\n",
    "        pred_start = np.argmax(logits_start, axis=1).flatten()\n",
    "        pred_end = np.argmax(logits_end, axis=1).flatten()\n",
    "\n",
    "        fin_targets_st.extend(label_start.tolist())\n",
    "        fin_outputs_st.extend(pred_start.tolist()) \n",
    "        fin_targets_ed.extend(label_end.tolist())\n",
    "        fin_outputs_ed.extend(pred_end.tolist())  \n",
    "        \n",
    "    return fin_outputs_st, fin_targets_st, fin_outputs_ed, fin_targets_ed\n",
    "\n",
    "\n",
    "def fit(train_dataloader, valid_dataloader, EPOCHS=3):\n",
    "    bert_QA=Bert_QA() #model\n",
    "    bert_QA=bert_QA.to(device)\n",
    "    loss_fn = loss_func #loss\n",
    "    optimizer = torch.optim.AdamW(bert_QA.parameters(),lr=lr) #optimizer\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        print(f\"EPOCHS:{i+1}\")\n",
    "        print('TRAIN')\n",
    "        train_one_epoch(train_dataloader, bert_QA, optimizer, device, loss_fn)\n",
    "        print('EVAL')\n",
    "        outputs_st, targets_st, outputs_ed, targets_ed = eval_one_epoch(valid_dataloader, bert_QA,  device, loss_fn)\n",
    "        targets_st, targets_ed = np.array(targets_st), np.array(targets_ed)\n",
    "        auc_st = accuracy_score(targets_st, outputs_st)\n",
    "        auc_ed = accuracy_score(targets_ed, outputs_ed)\n",
    "        auc = (auc_st + auc_ed)/2\n",
    "        \n",
    "        print(f\"auc_st;{auc_st}\") \n",
    "        print(f\"auc_ed;{auc_ed}\") \n",
    "        print(f\"auc;{auc}\")  \n",
    "    \n",
    "fit(train_dataloader, valid_dataloader)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
